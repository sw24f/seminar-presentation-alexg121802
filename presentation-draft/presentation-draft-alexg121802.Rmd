
---
title: "Effect sizes for paired data should use the change score variability rather than the pre-test variability:\\


A Paper by Dr. Scott Dankel and Dr. Jeremy P. Loenneke"
author: "Alex Gould"
date: "`r Sys.Date()`"
bibliography: pres.bib
output: 
  beamer_presentation:
    keep_tex: true  
    pandoc_args: [
      "--variable", "aspectratio=169"  # Set aspect ratio directly
    ]
header-includes:
  - \usepackage{amsmath}
  - \usepackage{graphicx}
  - \usepackage{booktabs}
  - \usepackage{setspace}
  - \usepackage{float}
  - \usepackage{hyperref}
  - \setbeamersize{text margin left=0.25in, text margin right=0.25in}
---


# Full Citation
Dankel, SJ and Loenneke, JP. Effect sizes for paired data should use the change score variability rather than the pre-test variability. J Strength Cond Res 35(6): 1773–1778, 2021—}


# What is effect size ?
  - Statistic that provides an overall measure for magnitude of change (@dankeleffect)
  - Differs from a T-statistic because sample size is not included
  - Quantifies how much the mean of the post-treatment differs from the mean of the baseline score
  in terms of a certain standard deviation.
    - Specifically, they are looking at this comparison from the lens of meta-analyses for exercise science and sports medicine


# Authors' Aims

## Aim 1
- To convince the audience through analysis that baseline and post-test standard deviations (study sample measures of variability) don't tell the full story on the overall variability of the intervention. 

## Aim 2
- To convince the readers that the heterogeneity of the study sample can play a part in unintentionally influencing effect size measurements.

## How would one describe the two types of aforementioned variability?


# Variability of the Study Sample
- Any measure of difference between subjects in a given treatment group
- Represented by the Baseline and Post-treatment Standard Deviation. 
- @dankeleffect and his team claim that the use of this type of variability in paired-sample studies is useless as it has nothing to do with the treatment itself


# Variability of the Intervention
- Any measure of difference between baseline and post-treatment measure
- Represented in this case by the Standard Deviation of Change Scores (I will elaborate on this later)
- Dankel and his team prefer this method of assessing variability 


# Dr. Scott Dankel
- Professor at Rowan University, a public research university in New Jersey
- Attended the University of Mississippi to pursue a Masters and PhD in Exercise Science
- Research Interests include acute and chronic adaptations to blood flow restricted exercise  [@dankelintro]


# Dr. Jeremy Paul Loenneke
- Professor at The University of Mississippi
- Attended Southeast Missouri State for his Bachelors and Masters in Nutrition and Exercise Science
- Eventually got his PhD in Exercise Physiology at the University of Oklahoma
- Research Discipline is in Skeletal Muscle Plasticity [@loennekeintro]


- Regarding the disciplines of the authors, this paper was published in The Journal of Strength and Conditioning Research. This is a good example of the use of statistics as an interdisciplinary tool 


# Introduction

## Specific Effect Size Measures

The author's claim that the common effect size measures listed below are used exhaustively in meta-analyses in the exercise science discipline. 

  - Cohen's d (@cohend)
  - Hedge's g (@cohend)
  - Glass delta (@glass-mcg)
  
  
  - Each use some combination of baseline standard deviation and post-treatment standard deviation.
  - Measures of variability of the study sample


# Paired Data vs. Independent Data

## Independent Data
  - Data collected through an Independent design
    - Each subject is only measured once
    - Subjects are allocated into a baseline group and a post-treatment group
    - Study sample variability is more important 
    - The pooled standard error is the way to assess this variability

## Paired Data
  - Data that is collected through a Paired Sample design 
      - Same subject is assessed at both time points. 
      - Since its based on the same subject, this data is not independent
      - In this type of Design, study sample variability is irrelevant
      - Variability of assessed by standard error of the change scores

Since most meta-analysis data is paired, Dankel's analysis focuses on primarily paired-sample designs. Therefore, the authors believe that intervention variability is the best measure for this specific analysis. 


# Methods 

## Preliminary measures
$$
M_{\text{change}}=M_{\text{post}}-M_{\text{bsl}}=\text{Difference between means}
$$
$$
SD_{bsl}={\text{Standard Deviation of the baseline group in an independent design}}
$$
$$
SD_{post}={\text{Standard Deviation of the posttreatment group in an independent design}}
$$
$$
n_{bsl}= \text{The sample size of the baseline group}
$$
$$
n_{post}= \text{The sample size of the posttreatment group}
$$

$$
SD_\text{pooled}=\sqrt{\frac{(n_{\text{bsl}}-1)SD_{\text{bsl}}^2+(n_{\text{post}}-1)SD^2_{\text{post}}}{
n_{\text{bsl}}+n_{\text{post}}-2}}
$$

# Calculations of Common Effect Size measures 

$\text{Cohen's } d= \frac{M_\text{change}}{SD_\text{pooled}}$

$\text{Glass's } \delta = \frac{M_\text{change}}{SD_{\text{bsl}}}$

$\text{Hedge's }g= C*\frac{M_\text{change}}{SD_\text{pooled}}$

Where $C$ is a factor depending on $n$ multiplied to account for small sample sizes

We can calculate an Independent $T$ test statistic: 

$T_\text{indep}= \frac{M_{\text{change}}}{\sqrt{SD_{\text{pooled}}(\frac{1}{n_\text{bsl}}+\frac{1}{n_{\text{post}}})}}$

# Calculations of the "Appropriate Effect Size" Measure
- Dankel and his team believe that the appropriate effect size measure for this study is Cohen's $d_z$
- I will outline the components and then the equation reported in the literature 

$r =${sample correlation between baseline and post measures}

$SD_\text{change}=\sqrt{SD_{\text{bsl}}^2+SD_{\text{post}}^2-2rSD_{\text{bsl}}SD_{\text{post}}}$

Cohen's $d_{z}= \frac{M_\text{change}}{SD_{change}}$

This also manifests in their calculation of a Paired $T$ statistic and resultingly the P-value: 

$n^*=${number of subjects in the paired design}

$T_{\text{paired}}=\frac{M_{\text{change}}}{\frac{SD_{\text{change}}}{\sqrt{n^*}}}$

# Analysis and Procedure
## Figure 1
- Describes a dataset with two interventions which have equivalent pre and post scores
- One intervention has a pre-post correlation of 0.99 and the other one has a pre-post correlation of -0.99.
- Seek to prove that pre and post scores dont tell the full story on intervention variability

## Figure 2
- Describes a dataset with two interventions such that one intervention has a higher pre and post standard deviation than intervention 2 
- They are both correlated the same way
- Seek to prove that heterogeneity of groups can have a profound effect on Common 
effect size measures but not on the measure that they define as the "appropriate" effect size

# Figure 1
Graph A shows that both interventions have the same pre and post means and standard deviations. Graph B shows the graph between pre and post, outlining opposite correlations, and Graph C is a representation of the standard error for each of the intervention groups, showing high variability for intervention 2 and low variability for intervention 1. Supports Aim 1. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{fig1.png}
\end{figure}

# Figure 2
Graph A shows that each intervention has the same pre and post mean but different pre-post standard deviations. Graph B shows the graph between pre and post, outlining the same correlations but different pre/post variabilities, and Graph 3 illustrates the variability further by showing that the intervention variability is the same. Supports Aim 2. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{fig2.png}
\end{figure}



# Results (Dankel et. al)
\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{figresult1.png}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{figresult2.png}
\end{figure}

- Respective to the figures above
- Top attempted to prove aim 1 
- Bottom attempted to prove aim 2 
- In both, 11.618 is mentioned as the "Appropriate effect size" even
though effect sizes rarely range over 4. 
- This produces an independent t-statistic of 36.74 and a p- value of $< 0.001$
- Standard Deviation of the change scores is suspiciously low as well

# My Results 

- The literature displayed the same $SD_\text{change}$ equation as was used in
the methods section above but did not use it. 
- Calculated 11.618 as their "appropriate effect size" 
- From the Literature: $M_\text{change}=3$, $SD_\text{pre}=6.05$, $SD_\text{post}=6.03$, 
$r=0.99$, $n^*=10$. 
- $SD_{\text{change}}=\sqrt{(6.05)^2+(6.03)^2-2(0.99)(6.05)(6.03)}\approx 0.8544 \neq 0.2582$
- Cohen's $d_z= \frac{3}{0.8544}\approx3.52$
- Paired $T$ statistic (formula) = $\frac{M_{\text{change}}}{\frac{SD_\text{change}}{\sqrt{n^*}}}$
-  Paired $T$ statistic (applied) = $\frac{3}{\frac{.8544}{\sqrt{10}}}= \frac{3}{0.2702}=11.10$
p-value $ < 0.001$

# Discussion

- Calculation error was due to reporting the standard error of the change scores as 
the Standard Deviation of the change scores

- Significance is the same with or without the error, although, it is important to note
that the standard error and the standard deviation are not interchangeable by any means

- Impossible to quantify variability of actual intervention when using pre-post scores 

- Figure 1: Intervention 1 produces positive effect where intervention two does not 

- Common effect size measures are reliant on the heterogeneity of the study population 

- Figure 2 shows identical results in 2 diff groups with the only difference being that intervention 1 used a more heterogeneous population 



# 
\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{any-questions.jpeg}
\end{figure}
[@anyquest]

# References 



